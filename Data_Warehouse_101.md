# Data Warehouse

## QUICK Summary
- There are three different types of source data systems: application data, third-party data and manual data.
- To store and process data for analytics, you need a thing called data warehouse.
- The process to move data from source to destination is called Extract & Load.
- Load data incrementally can help you increase the performance of your EL process.

## 1. Three different data source
   1. Data coming directly out of your main application (application database)
   2. Data coming from Customer Relationship Management (CRM), Content Management System (CMS), Enterprise Resource Planning (ERP), or marketing systems
   3. Manual data created by employees and other systems  


            Example:
            For instance, if you're a restaurant and want to analyze orders/waitress ratio efficiency (which hour of the week the staff is most busy vs most free), you need to combine your sales data (from POS system) with your staff duty data (from HR system).

## 3. Load data incrementally
   - Most business use cases just need a daily refresh of analytics data.
   - A pipeline that runs after midnight and finishes before people get to work.
   - How? [Click here](###-Incremental-Transform-in-aciton)

# Four Reasons TO Get A Data Warehouse
1. You need to analyse data from different sources
2. You need to separate analytical from transactional data
3. Your original data source isnt suitable for querying (No SQL, for instance!)
4. To improve the performance of your most-used queries (think: transformed summary tables!) 

# Transactional vs Analytics DBs
Transactional databases  
optimized for fast, short queries with high concurrent volume  

Analytics database  
optimized for long-running, resource-intensive queries  

|        | Transactional DBs           | Analytics DBs               |  
| -      | -                           | -                           |  
| -      | many simple queries         | few heavy queries           |  
|Data    | Manay single-row writes     | Few large batch imports     |   
|        | Current, single data        | Years of data, many sources |
|Queries | Generated by user activites | Generated by large reports  |  
|        | <1s response time           | Queries run for hours       |
|        | short queries               | Long, complex queries       |   
| -      | -                           | -                           |  
|workload| SELECT * FROM products<br>WHERE id = 123 | SELECT category_name, count(*)<br>as num_products<br>FROM products GROUP BY 1 |  




--- I dont understand this ---  
Quick note:  

Columnar storage engine: Instead of storing data row by row on disk, analytical databases group columns of data together and store them.

Compression of columnar data: Data within each column is compressed for smaller storage and faster retrieval.

Parallelization of query executions: Modern analytical databases are typically run on top of thousands of machines. Each analytical query can thus be split into multiple smaller queries to be executed in parallel amongst those machines (divide and conquer strategy).

## Aware of different between ELT vs ETL
ELT - transform inside warehouse  
ETL - transform outside warehouse  
*Storage Space is **cheap** in the Cloud era!*  
*"dump first, transform later"*  

### Host unstructured data - Data Lakes
- where all data stored before entry data warehouse
- upload all manner of unstructured data

### Data Transformation
1. Data clearning
2. Aggregate data (transaction counts by diff region and category, and store into a new table)
3. Pre-processing (calculate a ratio, a trend)

#### One-step Transformation
        Example:
        Imagine that we're running a hotel booking website, 
        and want to create a summarization of daily bookings according to a few dimensions. 
        In this case, we want to look at dimensions like country, 
        as well as the platform on which the booking was made.
        transform 'booking' and 'countries_code' into 'bookings_daily' table

![figure 1][1]

```sql
-- Transform: Summarize bookings by country and platform
BEGIN;
DROP TABLE IF EXISTS bookings_daily;
CREATE TABLE bookings_daily (
 date_d date,
 country_name varchar,
 platform varchar,
 total integer
);
INSERT INTO bookings_daily (
 date_d, country_name, platform, total
)
SELECT
 ts::date as date_d,
 C.country_name,
 platform,
 count(*) as total
FROM bookings B
LEFT JOIN countries C ON B.country_code = C.country_code
GROUP BY 1
COMMIT;
```

To deploy the above SQL code to production, we set up a daily cron job that runs the SQL file in PostgreSQL:

    $ psql transforms/bookings_daily.sql


#### multi-step Transformation - Directed Acyclic Graph Workflow
- dependency + specific order  
![directed_acyclic_graph.png][2]  


### Incremental Transform in aciton
![incremental_transform.png][3]  
```sql
-- Example
destination: bookings_daily --result table
incremental:
 enabled: true
 column: date_d -- use column date_d for incremental

SELECT
 ts::date as date_d,
 C.country_name,
 platform,
 count(*) as total
FROM bookings B --source table
LEFT JOIN countries C ON B.country_code = C.country_code
WHERE [[ ts::date > {{max_value}} ]] --this is added to the code, only pull the latest value. 
GROUP BY 1
```

*Data analyst map between business logic and data logic*  

## Data modelling example
![model_example.png][4]  

## Multi-table query
![multi_table_query.png][5]  

# Frank Bien's Three Waves of data analytics

In the first wave of data analytics, companies developed and sold monolithic stacks — that is, an all-in-one solution that came with data warehouse, data transformer, data cube solution, and visualization suite. This approach evolved out of technical necessity as much as anything else.

In this 'second wave' of business intelligence, data cubes and Cognoslike stacks continued to evolve, but new tools championed a 'selfservice' orientation. Tools like Tableau gave business users beautiful dashboards and visualizations with not a line of code in sight. These tools were in turn fed by data exports drawn from the earlier firstwave environment. The basic idea was that analysts and business users would download datasets from these central data systems, and then load these datasets into tools that they could install on their own computers.

Massively parallel processing (MPP) data warehouses
   - cloud vendor pricing model
   - scale up to hundreds or thousands of machines as is needed for the task at hand
columnar storage architecture



cube-oriented systems
decentralized Tableau-type analytical workflows

|vs|||
|-|-|-|
|SQL vs Non-SQL | Non-SQL: Tableau, PowerBI, Sisense |SQL: Holistics, Looker,Mode, Redash, Metabase|
|Embedded Datastore vs External Datastore|Embedded: MicroStrategy,Tableau, PowerBI, Sisense|External: Holistics, Looker,Metabase, Redash
|In-memory vs Indatabase |In-memory: Tableau,MicroStrategy, Sisense,PowerBI, etc.|In-database: Holistics, Looker, Redash, Metabase, etc.|
|Modeling vs nonmodeling BI tools |Non-modeling: Tableau, Mode,Redash|Modeling: Qlik, PowerBI,Looker, Holistics|

# Data maturity model
Emily Schario of GitLab, argues that all organizations go
through the same, three levels of data analysis:

1. Reporting — This is the lowest level. 
   1. ‘how many new users visited our website last week?’ and 
   2. ‘how many leads did we capture this month?’
   3. if you do not collect data or if you do not have the cultural expectation of using it, you’re not going to base your decisions on facts.
2. Insights — Insights is the next level above reporting.
   1. insights are about understanding relationships between facts
   2. implies combining data from multiple sources
   3. the number of new customers who cancelled their subscription this month is a reporting metric.
   4. If we combine 2 data in sales CRM, we might learn that we have been targeting a bad subsegment of our market.
   5. This observation is an insight, and can lead to behavioral change among sales and product
3. Predictions — Predictions come after insights.
   1. statistical analysis and machine learning.
   2. after increasingly understands the relationships between various metrics, you may begin to make informed business decisions to drive outcomes that you desire
   3. eg, Facebook discover that users who add at least seven friends in their first 10 days are the most likely to stick around.


---
[1]:db_warehouse_img/bookings_daily_transform.png  
[2]:db_warehouse_img/directed_acyclic_graph.png  
[3]:db_warehouse_img/incremental_transform.png  
[4]:db_warehouse_img/model_example.png  
[5]:db_warehouse_img/multi_table_query.png  

https://towardsdatascience.com/announcing-pycaret-2-0-39c11014540e